{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Overview\n",
    "\n",
    "During this week's lessons, you will learn how the vector space model works in detail, the major heuristics used in designing a retrieval function for ranking documents with respect to a query, and how to implement an information retrieval system (i.e., a search engine), including how to build an inverted index and how to score documents quickly for a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrases and Concepts\n",
    "\n",
    "Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
    "\n",
    "* Term frequency (TF)\n",
    "* Document frequency (DF) and inverse document frequency (IDF)\n",
    "* TF transformation\n",
    "* Pivoted length normalization\n",
    "* BM25\n",
    "* Inverted index and postings\n",
    "* Binary coding, unary coding, gamma-coding, and d-gap\n",
    "* Zipf’s law "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Objectives\n",
    "\n",
    "After you actively engage in the learning experiences in this module, you should be able to:\n",
    "\n",
    "* Explain what TF-IDF weighting is and why TF transformation and document length normalization are necessary for the design of an effective ranking function.\n",
    "* Explain what an inverted index is and how to construct it for a large set of text documents that do not fit into the memory.\n",
    "* Explain how variable-length encoding can be used to compress integers and how unary coding and gamma-coding work.\n",
    "* Explain how scoring of documents in response to a query can be done quickly by using an inverted index.\n",
    "* Explain Zipf’s law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guiding Questions\n",
    "\n",
    "Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
    "\n",
    "* What are some different ways to place a document as a vector in the vector space?\n",
    "    1. Represent words as bit-vectors \\\\(\\{w_i, ..., w_n\\} \\ w_i \\in {0, 1}\\\\) where 1 for mathced word to query and 0 for unmatched word to query.\n",
    "    2. Represent words as matched words count vectors \\\\(c(W_i, d)\\\\) .\n",
    "    3. Represent words as penalized matched words count vectors using IDF function \\\\(c(W_i, d)*IDF(W_i)\\\\).\n",
    "<br><br>\n",
    "* What is term frequency (TF)?<br>\n",
    "Count of word occurencess in the specific document.\n",
    "<br><br>\n",
    "* What is TF transformation?<br>\n",
    "Transform TF value into desired scale, such as:\n",
    "    1. Linear scale difined by word count.\n",
    "    2. Sublinear scale, i.e Logarithmic scale and BM25.\n",
    "    3. Bit scale.\n",
    "<br><br>\n",
    "* What is document frequency (DF)?<br>\n",
    "Document frequency is sum of documents contain term \\\\(t_i\\\\). \n",
    "<br><br>\n",
    "* What is inverse document frequency (IDF)?<br>\n",
    "Inverse document frequency is logarithm of number of document devided by document frequency.\n",
    "<br><br>\n",
    "* What is TF-IDF weighting?<br>\n",
    "TF-IDF weighting tells us that in large document collections, common words should have low rank value because they are tend to be biased by statistical point of view and rare words should have high rank value because they intuitively can be used to differentiate document by topic.\n",
    "<br><br>\n",
    "* Why do we need to penalize long documents in text retrieval?<br>\n",
    "In the case if bit-vector representation \\\\(\\{w_i, ..., w_n\\} \\ w_i \\in {0, 1}\\\\) not staisfy information need, we may used word count representation \\\\(c(W_i, d)\\\\). Then statistically speaking, we expected that TF should be uniformly distributed over related documents. If not uniformly distributed, then very high TF in few documents could skewed word distribution. In the skewed distribution, even conventional tf-idf function \\\\(c(W_i, d)*IDF(W_i)\\\\) may not helpful in order to generate fair relevance rank.\n",
    "<br><br>\n",
    "* What is pivoted document length normalization?<br>\n",
    "Pivoted length normalizer help us to normalize document length. The basic normalization work in two steps: First, it will normalize document length by divide document frequency \\\\(DF\\\\) with average document frequency \\\\(\\overline{DF}\\\\) and second, penalized normalized document length over desired upper bound \\\\(b\\\\).\n",
    "![pivoted-length-normalizer](images/pivoted-length-normalizer.png)<br>\n",
    "Then, we can impliment the document length normalization in TF-IDF calculation. In the case if we used logaithmic TF-IDF, then the our document vector will calculated by this formula:<br>\n",
    "$$f(q, d) = \\sum_{w \\in q \\cap d} c(w,q)*\\frac{ln[1+ln[1+c(w,d)]]}{1-b+b\\frac{|d|}{\\overline{dl}}}*log\\frac{M+1}{df(w)}$$\n",
    "<br><br>\n",
    "* What are the main ideas behind the retrieval function BM25?<br>\n",
    "Basically, BM25 help us to penalize very high TF value by using upper bound value \\\\(k\\\\). It can also be used with document length normalization:<br>\n",
    "$$f(q, d)=\\sum_{w \\in q \\cap d} c(w,q)*\\frac{(k+1)c(w,d)}{c(w,d)+k(1-b+b \\frac{|d|}{\\overline{dl}})}*log \\frac{M+1}{df(w)}$$\n",
    "where:\n",
    "$$\\eqalign{\n",
    "b &\\in [0,1]\\\\\n",
    "k_1, k_3 &\\in [0, +\\infty)\n",
    "}$$\n",
    "<br><br>\n",
    "* What is the typical architecture of a text retrieval system?<br>\n",
    "A typical architecture of a text retrieval consisted by four components: **Tokenizer**, **Indexer**, **Scorer**, and **Feedback**.\n",
    "\n",
    "![typical architecture](images/typical-architecture.png)\n",
    "<br><br>\n",
    "* What is an inverted index?<br>\n",
    "Inverted index is indexing strategy to provide fast document lookup and aggregation process such as TF-IDF. Basically, it consisted by two elements: **Dictionary** and **Postings**. An image below is an example of inverted index which suited for TF-IDF model:\n",
    "![inverted index](images/inverted-index.png)\n",
    "<br><br>\n",
    "* Why is it desirable to compress an inverted index?<br>\n",
    "If we working with large corpus, then **memory-based method** is not feasible. Another way is using **sort-based method** with following steps:\n",
    "    1. Collect local \\\\((termID, docID, freq)\\\\) tuples.\n",
    "    2. Sort local tuples (to make runs)\n",
    "    3. Pair-wise merge runs\n",
    "    4. Output inverted index\n",
    "All step above could be illustrate in an image below:\n",
    "![sort based method](images/sort-based-method.png)<br>\n",
    "The critical part of this method is **ensure that local sort and merge sort** fit in memory. To do that, we can create virtual memory with enough space. **In the case that virtual memory does not satisfy performance requirement, then we should implement compression algorithm to compress tuple(termID, docID, freq)**.\n",
    "Another reason we need compression algorithm is **distribution of term frequency (TF) tend to follow zipf's law** that tells us that most of words have low frequency, but only few words have very high frequency. This fact will caused distribution of trem frequencies tend to be skewed. Furthermore, **dirtibution of a term in documents with general topic** may also skewed, few words may occured in most documents, while most words occured in few documents.\n",
    "<br><br>\n",
    "* How can we create an inverted index when the collection of documents does not fit into the memory?<br>\n",
    "We can implement coding theory and information theory to encode distribution of term frequncy and document id:\n",
    "    1. **TF compression**: Fewer bits for high frequency term and more bits for low frequncy term.\n",
    "    2. **Doc ID compression**: Use d-gap compression that store first mathced document follwed by distance to next Doc ID. For example, term found in Doc ID [d1, d4, d9, d10] can be compressed as {d1: [3, 5, 1]}.\n",
    "<br><br>\n",
    "* How can we leverage an inverted index to score documents quickly?<br>\n",
    "The idea of how we can leverage an inverted index to score documents quickly is by using **data aggregation** into inverted index. Formally, we can computer score function of each document by using this formula:\n",
    "$$f(q,d) = f_a(h(g(t_1,d,q), ..., g(t_k,d,q)), f_d(d), f_q(q))$$\n",
    "where:\n",
    "$$\\eqalign{\n",
    "f_a &: \\text{Final score adjustment}\\\\\n",
    "f_d(d) &: \\text{Document score adjustment}\\\\\n",
    "f_q(q) &: \\text{Query score adjustment}\\\\\n",
    "h &: \\text{Weight aggregation}\\\\\n",
    "g(t_i,d,q) &: \\text{Weight of mathed query term in d}\n",
    "}$$<br>\n",
    "A general algorithm to implement score function in order to ranking documents decribed in folloing steps:\n",
    "    1. Pre-compute \\\\(f_d(d)\\\\) and \\\\(f_q(q)\\\\).\n",
    "    2. Maintain a score accumulator for each \\\\(d\\\\) to compute \\\\(h\\\\).\n",
    "    3. For each query term \\\\(t_i\\\\):\n",
    "        - Fetch the inverted list \\\\(\\{(d_1,f_1),..,(d_n,f_n)\\}\\\\)\n",
    "        - For each entry \\\\((d_j,f_j)\\\\), computer \\\\(g(t_i,d_j,1)\\\\), and update score accumulator for doc \\\\(d_i\\\\) to incrementally compute \\\\(h\\\\)\n",
    "    4. Adjust the score to compute \\\\(f_a\\\\) and sort\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Readings and Resources\n",
    "\n",
    "The following readings are optional:\n",
    "\n",
    "* C. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan & Claypool Publishers, 2016. Chapter 6 - Section 6.3, and Chapter 8.\n",
    "* Ian H. Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images, Second Edition. Morgan Kaufmann, 1999.\n",
    "* Stefan Büttcher, Charles L. A. Clarke, Gordon V. Cormack: Information Retrieval - Implementing and Evaluating Search Engines. MIT Press, 2010."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
