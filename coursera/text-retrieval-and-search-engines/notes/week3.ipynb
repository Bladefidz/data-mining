{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Overview\n",
    "\n",
    "During this week's lessons, you will learn how to evaluate an information retrieval system (a search engine), including the basic measures for evaluating a set of retrieved results and the major measures for evaluating a ranked list, including the average precision (AP) and the normalized discounted cumulative gain (nDCG), and practical issues in evaluation, including statistical significance testing and pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrases and Concepts\n",
    "\n",
    "Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
    "\n",
    "- Cranfield evaluation methodology\n",
    "- Precision and recall\n",
    "- Average precision, mean average precision (MAP), and geometric mean average precision (gMAP)\n",
    "- Reciprocal rank and mean reciprocal rank\n",
    "- F-measure\n",
    "- Normalized discounted cumulative Gain (nDCG)\n",
    "- Statistical significance test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Objectives\n",
    "\n",
    "After you actively engage in the learning experiences in this module, you should be able to:\n",
    "\n",
    "- Explain the Cranfield evaluation methodology and how it works for evaluating a text retrieval system.\n",
    "- Explain how to evaluate a set of retrieved documents and how to compute precision, recall, and F1.\n",
    "- Explain how to evaluate a ranked list of documents.\n",
    "- Explain how to compute and plot a precision-recall curve.\n",
    "- Explain how to compute average precision and mean average precision (MAP).\n",
    "- Explain how to evaluate a ranked list with multi-level relevance judgments.\n",
    "- Explain how to compute normalized discounted cumulative gain.\n",
    "- Explain why it is important to perform statistical significance tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guiding Questions\n",
    "\n",
    "Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is evaluation so critical for research and application development in text retrieval?\n",
    "\n",
    "- Text retrieval is empirical task, so we need to measure the text retrieval result quality based on user, not subjectively measured.\n",
    "- We need to understand what actual utility of text retrieval system from user perspective. To do that, we need to evaluate each possible utilty and measure them through user study.\n",
    "- Measure actual utility on only one system and method is not enough. We need to do evaluation on different systems and methods to reveal exact utility to user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the Cranfield evaluation methodology work?\n",
    "\n",
    "Let:\n",
    "- \\\\(D\\\\) is set of documents \\\\(\\{d_1, d_2, ..., d_n\\}\\\\)\n",
    "- \\\\(Q\\\\) is set of queries \\\\(\\{q_1, q_2, ..., q_n\\}\\\\)\n",
    "- \\\\(S\\\\) is set of systems \\\\(\\{s_1, s_2, ..., s_n\\}\\\\)\n",
    "- \\\\(R\\\\) is set of relevance judgement by users for each system in \\\\(S\\\\) for each document in \\\\(D\\\\) should be have relevance judgement in \\\\(J\\\\), such that \\\\(R_{Si} = \\{d_i \\rightarrow j_i, ..., d_n \\rightarrow j_i \\ | \\ d_i \\in D \\ , \\ j_i \\in J\\}\\\\)\n",
    "\n",
    "Suppose:\n",
    "- We have two systems \\\\(S = \\{A, B\\}\\\\)\n",
    "- We want to match query \\\\(Q_i\\\\) to each document in \\\\(D\\\\) using each system\n",
    "- We have boolean judgement, such that \\\\(J = \\{+, -\\}\\\\)\n",
    "- \\\\(R_A\\\\) return \\\\(\\{d_2 \\rightarrow +, d_1 \\rightarrow +, d_4 \\rightarrow -\\}\\\\)\n",
    "- \\\\(R_B\\\\) return \\\\(\\{d_1 \\rightarrow +, d_4 \\rightarrow -, d_3 \\rightarrow -, d_5 \\rightarrow +, d_2 \\rightarrow +\\}\\\\)\n",
    "\n",
    "Then:\n",
    "- By using **precission**, we decide that \\\\(R_A\\\\) better that \\\\(R_B\\\\), since \\\\(2/3 > 3/5\\\\)\n",
    "\n",
    "![cranfield](images/cranfield.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we evaluate a set of retrieved documents?\n",
    "\n",
    "- Using **Precision** to evaluate degree of relevant from set of retrieved documents.\n",
    "- Using **Recall** to evaluate relevant ratio of retrieved against not retreived.\n",
    "- Using **F1** to combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you compute precision, recall, and F1?\n",
    "\n",
    "Consider this matrix:\n",
    "\n",
    "Doc \\ Action | Retrieved | Not Retrieved |\n",
    "-------------|-----------|---------------|\n",
    "Relevant     |     a     |       b       |\n",
    "Not Relevant |     c     |       d       |\n",
    "\n",
    "$$Precision = \\frac{a}{a+c}$$\n",
    "$$Recall = \\frac{a}{a+b}$$\n",
    "$$\\eqalign{\n",
    "    F_{\\beta} &= \\frac{1}{\\frac{\\beta^2}{\\beta^{2+1}}\\frac{1}{R} + \\frac{1}{\\beta^2+1}\\frac{1}{P}}\\\\\n",
    "              &= \\frac{(\\beta^2+1)P*R}{\\beta^2P+R}\\\\\n",
    "              \\text{if } \\beta = 1\\\\\n",
    "    F_1       &= \\frac{2PR}{P+R}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we evaluate a ranked list of search results?\n",
    "\n",
    "Let:\n",
    "- Users walking trough retrieved documents and judge each of document.\n",
    "- Compute precision-recall each level on set of retrieved document, such that for \\\\(N\\\\) retrieved documents, we have \\\\(N\\\\) precission-recall. \n",
    "\n",
    "Suppose:\n",
    "- Judge \\\\(J\\\\) is binary judgement, such that \\\\(J = \\{+, -\\}\\\\)\n",
    "- Precision-recall computation result:\n",
    "\n",
    "Doc, judge  | Precision | Recall |\n",
    "------------|-----------|--------|\n",
    "\\\\(D_1+\\\\)  | 1/1       |  1/10  |\n",
    "\\\\(D_2+\\\\)  | 2/2       |  2/10  |\n",
    "\\\\(D_3-\\\\)  | 2/3       |  2/10  |\n",
    "\\\\(D_4-\\\\)  | 2/4       |  2/10  |\n",
    "\\\\(D_5+\\\\)  | 3/5       |  3/10  |\n",
    "\\\\(D_6-\\\\)  | 3/6       |  3/10  |\n",
    "\\\\(D_7-\\\\)  | 3/7       |  3/10  |\n",
    "\\\\(D_8+\\\\)  | 4/8       |  4/10  |\n",
    "\\\\(D_9-\\\\)  | 4/9       |  4/10  |\n",
    "\\\\(D_{10}-\\\\) | 4/10      |  4/10  |\n",
    "\n",
    "Then we got:\n",
    "- A precision-recall curve:\n",
    "![Precision Recall Curve](images/precision-recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you compute average precision? How do you compute mean average precision (MAP) and geometric mean average precision (gMAP)?\n",
    "\n",
    "For single search engine system and specific query,\n",
    "**Average precision** of ranked list \\\\(L\\\\):\n",
    "\n",
    "$$avg(L) = \\frac{1}{|Rel|}\\sum_{i=1}^n p(i)$$\n",
    "\n",
    "where:\n",
    "- Length of \\\\(L\\\\) is \\\\(n\\\\)\n",
    "- \\\\(Rel\\\\) is total relevant documents in the collections\n",
    "- $$p(i) = \\begin{cases}\n",
    "0,& \\text{if } D_i \\text{ is judged as not relevant}\\\\\n",
    "\\frac{\\sum_{rel}}{rank},& \\text{if } D_i \\text{ is judged as relevant}\n",
    "\\end{cases}$$\n",
    "- \\\\(\\sum_{rel}\\\\) is current total of judged relevance document in \\\\(i \\ rank\\\\)\n",
    "\n",
    "\n",
    "For multiple search engine system and multiple queries, **Mean Average Precision** (MAP) is arithmetic mean of all the average precisions over several queries or topics, Let \\\\(\\mathcal{L} = L_1, L_2, ..., L_m\\\\) be the ranked lists returned from running \\\\(m\\\\) different queries. Then we have:\n",
    "\n",
    "$$MAP(\\mathcal{L}) = \\frac{1}{m} \\sum\\limits_{i=1}^m avp(\\mathcal{L}_i)$$\n",
    "\n",
    "\n",
    "**geometric Mean Average Precision** (gMAP) enchance MAP capability to capture low ranked queries that far away from average value. We defined gMAP as:\n",
    "\n",
    "$$gMap(\\mathcal{L}) = \\big( \\prod\\limits_{i=1}^m avp(\\mathcal{L}_i) \\big)^{\\frac{1}{m}}$$\n",
    "\n",
    "or in log space as\n",
    "\n",
    "$$gMAP(\\mathcal{L}) = exp \\big( \\frac{1}{m} \\sum\\limits_{i=1}^m ln \\ avp(\\mathcal{L}_i) \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is mean reciprocal rank?\n",
    "\n",
    "Reciprocal rank is special case of MAP where there are always \\\\(r\\\\) relevant document on the entire collection, such that average precision will always has value equal to \\\\(\\frac{1}{r}\\\\) where \\\\(r\\\\) is the position (rank) of the single relevant document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is MAP more appropriate than precision at k documents when comparing two retrieval methods?\n",
    "\n",
    "Using precision at k documents in comparing two retrieval methods produce unfair measurement since each methods retrieved different \\\\(k\\\\) documents. MAP is more usefull in comparing two retrieval methods because MAP provide a way to measure total precision of each method relative to average precision. Thus, we may see that average precision is expected precision which can be achieved by single retrieval method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is precision at k documents more meaningful than average precision from a userâ€™s perspective?\n",
    "\n",
    "Since order of retrieved document represent probability of relevance, than user tend to consider most \\\\(k\\\\) document. Thus, user's perspective is subjective preferences. Also, in the case of question and answer search engine, top answer is always prefered to be right answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we evaluate a ranked list of search results using multi-level relevance judgments?\n",
    "\n",
    "Use **Cumulative Gain** (CG) and **Discounted Cumulative Gain** (DCG):\n",
    "\n",
    "Let:\n",
    "- \\\\(r_i\\\\) is **gain** of result \\\\(i\\\\)\n",
    "- \\\\(i\\\\) is **index of set of retrieved document**, such that \\\\(\\{d_1, d_2, ..., d_n\\}\\\\)\n",
    "\n",
    "Then:\n",
    "- $$CG(L) = \\sum\\limits_{i=1}^n r_i$$\n",
    "- $$DCG(L) = r_1 + \\sum\\limits_{i=2}^n \\frac{r_i}{log_2 i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you compute normalized discounted cumulative gain (nDCG)?\n",
    "\n",
    "Let:\n",
    "- \\\\(iDCG\\\\) is **ideal Discounted Cumulative Gain**\n",
    "\n",
    "Then:\n",
    "- $$nDCG(L) = \\frac{DCG(L)}{iDCG}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is normalization necessary in nDCG? Does MAP need a similar normalization?\n",
    "\n",
    "Because we need absolute measurement for all systems, while nDCG introduce ideal DCG as absolute measurement. By using absolute measurement, we can ensure comparability across queries.\n",
    "\n",
    "MAP can not use normalization, since ideal MAP is always 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it important to perform statistical significance tests when we compare the retrieval accuracies of two search engine systems?\n",
    "\n",
    "Statistical significance test provide a way to assess the variance in average precision scores across these different queries. If there's a big variance, that means the results could fluctuate according to different queries, which makes the result unreliable.\n",
    "\n",
    "One popular statistical signficance test is **Wilcoxon signed-rank test**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Readings and Resources\n",
    "\n",
    "- Mark Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval 4, 4 (2010), 247-375.\n",
    "- Diane Kelly, Methods for Evaluating Interactive Information Retrieval Systems with Users. Foundations and Trends in Information Retrieval 3(1-2): 1-224 (2009)\n",
    "- C. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan & Claypool Publishers, 2016. Chapter 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
