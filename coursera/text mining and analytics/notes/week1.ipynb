{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Overview\n",
    "\n",
    "During this week's lessons, you will learn the overall course design, an overview of natural language processing techniques and text representation, which are the foundation for all kinds of text-mining applications, and word association mining with a particular focus on mining one of the two basic forms of word associations (i.e., paradigmatic relations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Objectives\n",
    "\n",
    "After you actively engage in the learning experiences in this module, you should be able to:\n",
    "\n",
    "* Explain some basic concepts in natural language processing.\n",
    "* Explain different ways to represent text data.\n",
    "* Explain the two basic types of word associations and how to mine paradigmatic relations from text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts:\n",
    "\n",
    "* Part of speech tagging\n",
    "* Syntactic analysis\n",
    "* Semantic analysis\n",
    "* Ambiguity\n",
    "* Text representation, especially bag-of-words representation\n",
    "* Context of a word; context similarity\n",
    "* Paradigmatic relation\n",
    "* Syntagmatic relation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Readings and Resources\n",
    "\n",
    "The following readings are optional:\n",
    "\n",
    "* C. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining. ACM and Morgan & Claypool Publishers, 2016. Chapters 1-4, Chapter 13.\n",
    "* Chris Manning and Hinrich Sch√ºtze, Foundations of Statistical Natural Language Processing. MIT Press. Cambridge, MA: May 1999. Chapter 5 on collocations.\n",
    "* Chengxiang Zhai, Exploiting context to identify lexical atoms: A statistical view of linguistic context. Proceedings of the International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97), Rio de Janeiro, Brazil, Feb. 4-6, 1997, pp. 119-129.\n",
    "* Shan Jiang and ChengXiang Zhai, Random walks on adjacency graphs for mining lexical relations from big text data. Proceedings of IEEE BigData Conference 2014, pp. 549-554."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guiding Questions\n",
    "\n",
    "Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
    "\n",
    "* What does a computer have to do in order to understand a natural language sentence?<br>\n",
    "First step, do lexical analysis. Second step, do syntactic analysis. Third step, do semantic analysis or pragmatic analysis.\n",
    "<br><br>\n",
    "* What is ambiguity?<br>\n",
    "Ambiguity mean a word may has multiple meanings.\n",
    "<br><br>\n",
    "* Why is natural language processing (NLP) difficult for computers?<br>\n",
    "NLP is hard fr computer because word ambiguities and prerequisite common knowledge to understand the meaning of sentence.\n",
    "<br><br>\n",
    "* What is bag-of-words representation?<br>\n",
    "Bag-of-words is a data structure similar to linked list which contains word data type.\n",
    "<br><br>\n",
    "* Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?<br>\n",
    "Word-based representation more robust than syntactic and semantic analysis because has less constrains, so can be used in more general cases and scalable.\n",
    "<br><br>\n",
    "* What is a paradigmatic relation?<br>\n",
    "If word A and B can be substitute each other. For example, *cat* and *dog* can be subtituted because they are in same semantic class, animal.\n",
    "<br><br>\n",
    "* What is a syntagmatic relation?<br>\n",
    "If word A and B can be combined into new meaning. Furthermore, A or B may be phrase. For example, *car* and *drive* can be combined as *drive car* since they are semantically corelated.\n",
    "<br><br>\n",
    "* What is the general idea for discovering paradigmatic relations from text?<br>\n",
    "The idea of paradigmatic relations is word A and B can be subtitute if they have high **similar context**.\n",
    "![pardigmatic-relation](images/paradigmatic-relation.png)\n",
    "<br><br>\n",
    "* What is the general idea for discovering syntagmatic relations from text?<br>\n",
    "The idea of syntagmatic relations is word or phrase A and B can be combined if they statistically have high frequency to occurred in the same documents or in another word, **co-occurred of A and B**.\n",
    "![syntagmatic-relation](images/syntagmatic-relation.png)\n",
    "<br><br>\n",
    "* Why do we want to do Term Frequency Transformation when computing similarity of context?<br>\n",
    "We want to penalize high frequency of word occureance in a document by using logarithmic scale. This idea based on the assumption that in the well structured text, word with high occurence should be uniformly distributed and word with vey low occurence tend to be normaly distributed. For example in the collection of document that belong to 'english soccer' category, we know apriori that words such as 'Chelsea' and 'Manchester' typically occured with *k* frequency, so by using penalized logarithmic scale, we want to penalize high bias words such as 'Shoot', 'Pass', or 'The' (in the case we are not used stop words) into *k* frequency.\n",
    "<br><br>\n",
    "* How does BM25 Term Frequency transformation work?<br>\n",
    "BM25 try to do frequency smoothing regardless of frequency of word occurencess. In the other word, we want to make histogram of high frequency word look like nice uniform shape.<br>\n",
    "![BM25](images/bm25.png)\n",
    "<br><br>\n",
    "* Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?<br>\n",
    "Since TF only tells us local distribution, we want to know its TF relative to whole documents. In other word, we want to compare its vector relative to whole space.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
