{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Overview\n",
    "\n",
    "During this week's lessons, you will learn topic analysis in depth, including mixture models and how they work, Expectation-Maximization (EM) algorithm and how it can be used to estimate parameters of a mixture model, the basic topic model, Probabilistic Latent Semantic Analysis (PLSA), and how Latent Dirichlet Allocation (LDA) extends PLSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Objectives\n",
    "\n",
    "After you actively engage in the learning experiences in this module, you should be able to:\n",
    "\n",
    "* Explain what a mixture of unigram language model is and why using a background language in a mixture can help “absorb” common words in English.\n",
    "* Explain what PLSA is and how it can be used to mine and analyze topics in text.\n",
    "* Explain the general idea of using a generative model for text mining.\n",
    "* Explain how to compute the probability of observing a word from a mixture model like PLSA.\n",
    "* Explain the basic idea of the EM algorithm and how it works.\n",
    "* Explain the main difference between LDA and PLSA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrases and Concepts\n",
    "\n",
    "* Mixture model\n",
    "* Component model\n",
    "* Constraints on probabilities\n",
    "* Probabilistic Latent Semantic Analysis (PLSA)\n",
    "* Expectation-Maximization (EM) algorithm\n",
    "* E-step and M-step\n",
    "* Hidden variables\n",
    "* Hill climbing\n",
    "* Local maximum\n",
    "* Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guiding Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a mixture model?\n",
    "\n",
    "Mixture model is improved version of generative model to penalize common word by introducing another distribution called as background word distribution. The background word distribution used to quantitative fixed probability of common word in general collection, such as collection of English documents. Then mix the distribution of word in particular document with distribution of background model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general, how do you compute the probability of observing a particular word from a mixture model?\n",
    "\n",
    "Let:\n",
    "\n",
    "- $\\theta_d$ is the unkown topic model which is word distribution of particular document $d$.\n",
    "- $\\theta_B$ is the known background model.\n",
    "\n",
    "Constrain:\n",
    "\n",
    "- $p(\\theta_d) + p(\\theta_B) = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "- A mixture model of word $w$ is:\n",
    "$$p(w) = p(\\theta_B) p(w|\\theta_B) + p(w|\\theta_d)p(\\theta_d)$$\n",
    "\n",
    "\n",
    "![mixture model](images/mixture-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the general form of the expression for this probability?\n",
    "\n",
    "Let:\n",
    "\n",
    "- $d$ is the document.\n",
    "- $\\theta_d$ is the topic model of $d$.\n",
    "- $\\theta_B$ is background topic.\n",
    "\n",
    "Then:\n",
    "\n",
    "- For all word distribution in the document, optimize all parameters as $\\Lambda$, such that:\n",
    "$$\\Lambda = (\\{p(w|\\theta_d)\\}, \\{p(w|\\theta_B)\\}, p(\\theta_B), p(\\theta_d))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the maximum likelihood estimate of the component word distributions of a mixture model behave like?\n",
    "\n",
    "Given that:\n",
    "\n",
    "- Likehood function:\n",
    "$$\\eqalign{\n",
    "    p(d|\\Lambda) &= \\prod_{i=1}^{|d|} p(x_i|\\Lambda) = \\prod_{i=1}^{|d|}[p(\\theta_d)p(x_i|\\theta_d) + p(\\theta_B)p(x_i|\\theta_B)]\\\\\n",
    "    &= \\prod_{i=1}^M[p(\\theta_d)p(w_i|\\theta_d)+p(\\theta_B)p(w_i|\\theta_B)]^{c(w,d)}\n",
    "}$$\n",
    "\n",
    "- Maximum Likehood (ML) estimate is sum of likehood function for each word probability:\n",
    "$$\\Lambda^* = arg \\ max_{\\Lambda} \\ p(d|\\Lambda)$$\n",
    "constraint:\n",
    "$$\\sum_{i=1}^M p(w_i|\\theta_d) + \\sum_{i=1}^M p(w_i|\\theta_B) = 1$$\n",
    "or\n",
    "$$p(\\theta_d) + p(\\theta_B) = 1$$\n",
    "\n",
    "The ML estimate follow two rules:\n",
    "\n",
    "1. The mixture model is a linear system, thus one can find unknown distribution given by known distribution. For example:\n",
    "    - The problem of two words distribution of *text* and *the*.\n",
    "    - Given that background distribution ($\\theta_B$) are:\n",
    "    $$\\eqalign{\n",
    "        p(\"text\"|\\theta_B) &= 0.1\\\\\n",
    "        p(\"the\"|\\theta_B) &= 0.9\n",
    "    }$$\n",
    "    - While:\n",
    "    $$\\eqalign{\n",
    "        p(\\theta_d) &= 0.5\\\\\n",
    "        p(\\theta_B) &= 0.5\n",
    "    }$$\n",
    "    - The distribution is a linear system:\n",
    "    $$\\eqalign{\n",
    "        p(d|\\Lambda) &= p(\"text\"|\\Lambda) p(\"the\"|\\Lambda)\\\\\n",
    "                     &= [0.5*p(\"text\"|\\theta_d)+0.5*0.1]*[0.5*p(\"the\"|\\theta_d)+0.5*0.9]\n",
    "    }$$\n",
    "<br>\n",
    "2. The linear system follow algebra rule:\n",
    "    - The algebra rule: if $x+y = constant$, then $xy$ reaches maximum when $x=y$.\n",
    "    - The previous linear system can be solved by follow the algebra rule:\n",
    "    $$0.5*p(\"text\"|\\theta_d)+0.5*0.1 = 0.5*p(\"the\"|\\theta_d)+0.5*0.9$$\n",
    "    - So:\n",
    "    $$p(\"text\"|\\theta_d)=0.9 >> p(\"the\"|\\theta_d)=0.1$$\n",
    "    - The word distribution is inequality between probability of words, such that: if $p(w_1|\\theta_B)>p(w_2|\\theta_B)$, then $p(w_1|\\theta_d)<p(w_2|\\theta_d)$.\n",
    "<br><br>\n",
    "\n",
    "The ML estimate behaviors are:\n",
    " \n",
    "1. Higher frequency words get higher $p(w|\\theta_d)$. It's means that if a word occurs more frequently in the observed text data, it would also encourage the unknown distribution $\\theta_d$ to assign a somewhat higher probability to this word.\n",
    "2. There are exist $p(w_i|\\theta_d)>p(w_j|\\theta_B)$ and $p(w_j|\\theta_d)<p(w_i|\\theta_B)$ for any random word $\\{w_i, w_j\\}$ in the data. It's means each distribution try to bet high probability to any other word.\n",
    "3. The probability $p(\\theta)$ regulates the collaboration and competition between component models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In what sense do they “collaborate” and/or “compete”?\n",
    "\n",
    "The first behavior of Maximum Likehood Estimate of mixed model tells us that each word distribution is an inequality, such that:\n",
    "\n",
    "- Two distribution will **collaborate** with each other in order to maximize the constrains $p(\\theta_d)+p(\\theta_B)=1$. It's means, the colaboration try to find maximum value of $p(\\theta_d)$ and $p(\\theta_B)$.\n",
    "- If a distribution assigns high probability to word $w_i$ than word $w_j$, then another distribution do the opposite by assign high probability to word $w_j$ than word $w_i$. It's means two distribution will **compete** to each other in order to assign high probability in particular word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why can we use a fixed background word distribution to force a discovered topic word distribution to reduce its probability on the common (often non-content) words? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the basic idea of the EM algorithm? What does the E-step typically do? What does the M-step typically do? In which of the two steps do we typically apply the Bayes rule? Does EM converge to a global maximum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is PLSA? How many parameters does a PLSA model have? How is this number affected by the size of our data set to be mined? How can we adjust the standard PLSA to incorporate a prior on a topic word distribution? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is LDA different from PLSA? What is shared by the two models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Readings and Resources\n",
    "\n",
    "\n",
    "* C. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining. ACM and Morgan & Claypool Publishers, 2016. Chapter 17.\n",
    "* Blei, D. 2012. Probabilistic Topic Models. Communications of the ACM 55 (4): 77–84. doi: 10.1145/2133806.2133826.\n",
    "* Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. Automatic Labeling of Multinomial Topic Models. Proceedings of ACM KDD 2007, pp. 490-499, DOI=10.1145/1281192.1281246.\n",
    "* Yue Lu, Qiaozhu Mei, and Chengxiang Zhai. 2011. Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. Information Retrieval, 14, 2 (April 2011), 178-203. doi: 10.1007/s10791-010-9141-9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
